{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f766f780",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kachr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\kachr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import json\n",
    "import joblib\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from ConfigSchema import ConfigSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "271da2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('semi_automated_dataset_creation/processed_decomposed_dataset.jsonl', 'r') as f:\n",
    "    dataset = [json.loads(line) for line in f]\n",
    "\n",
    "data_pair = np.array([\n",
    "    (item['query'], item['S_a'] if item['label'] <= 0 else item['S_b'])\n",
    "    for item in dataset\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea2d9e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_schema = ConfigSchema()\n",
    "with open(\"config.cfg\", \"r\") as cfg:\n",
    "    config = {}\n",
    "    for line in cfg:\n",
    "        if line.strip() and not line.startswith(\"#\"):\n",
    "            key, value = line.strip().split(\"=\")\n",
    "            config[key.strip()] = value.strip()\n",
    "config_schema.from_dict(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "181eddc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(config_schema.model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a412729",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tokenizer.batch_encode_plus(\n",
    "    data_pair[:, 0].tolist(),\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    max_length=256,\n",
    "    return_tensors='np'\n",
    ")\n",
    "X = X['input_ids'] / tokenizer.vocab_size\n",
    "np.save('X.npy', X)\n",
    "print(\"X done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "302b4c2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.33034166e-01, 1.79566482e-01, 4.30478163e-01, ...,\n",
       "        1.00001319e+00, 1.00001319e+00, 1.00001319e+00],\n",
       "       [3.62759903e-02, 8.44747202e-03, 1.74093100e-03, ...,\n",
       "        1.00001319e+00, 1.00001319e+00, 1.00001319e+00],\n",
       "       [5.53932592e-03, 1.36979617e-01, 3.46669480e-02, ...,\n",
       "        1.00001319e+00, 1.00001319e+00, 1.00001319e+00],\n",
       "       ...,\n",
       "       [3.40140989e-02, 1.83984754e-03, 2.20346472e-01, ...,\n",
       "        1.00001319e+00, 1.00001319e+00, 1.00001319e+00],\n",
       "       [3.23015240e-01, 1.76269264e-02, 7.25387918e-05, ...,\n",
       "        5.85651827e-02, 1.74093100e-03, 1.13635315e-01],\n",
       "       [5.55251479e-02, 3.46207870e-03, 2.14319157e-02, ...,\n",
       "        1.01949975e-01, 5.55251479e-03, 4.08855008e-04]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.load('X.npy')\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ba6430",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = SentenceTransformer('all-mpnet-base-v2')\n",
    "Y = encoder.encode(data_pair[:,1], normalize_embeddings=True)\n",
    "np.save('Y.npy', Y)\n",
    "print(\"Y done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e161992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00895995, -0.12334388, -0.0325086 , ..., -0.05825622,\n",
       "         0.0237086 , -0.00698727],\n",
       "       [ 0.02767649,  0.02718943, -0.01895026, ...,  0.02996054,\n",
       "         0.01355811, -0.02394591],\n",
       "       [-0.02491825,  0.00840581, -0.0215272 , ...,  0.05550037,\n",
       "        -0.03574743, -0.00795398],\n",
       "       ...,\n",
       "       [ 0.03039899, -0.00654207,  0.00221696, ...,  0.00505765,\n",
       "         0.03097504, -0.00514259],\n",
       "       [-0.06168054,  0.13452576, -0.03894248, ...,  0.03692231,\n",
       "        -0.05549473, -0.03547874],\n",
       "       [ 0.0645332 , -0.00039842,  0.00253504, ..., -0.00615496,\n",
       "         0.02678013, -0.03398724]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = np.load('Y.npy')\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df4f82cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: X=(7295, 256), Y=(7295, 768)\n",
      "Test shape:  X=(1824, 256), Y=(1824, 768)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train shape: X={X_train.shape}, Y={Y_train.shape}\")\n",
    "print(f\"Test shape:  X={X_test.shape}, Y={Y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0bbf2dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def cosine_similarity_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Loss = 1 - cosine_similarity (averaged over batch)\n",
    "    \"\"\"\n",
    "    y_true = tf.math.l2_normalize(y_true, axis=1)\n",
    "    y_pred = tf.math.l2_normalize(y_pred, axis=1)\n",
    "    cosine_sim = tf.reduce_sum(y_true * y_pred, axis=1)  # batch of sims\n",
    "    return 1.0 - tf.reduce_mean(cosine_sim)  # final scalar loss\n",
    "\n",
    "def hybrid_loss(y_true, y_pred):\n",
    "    cosine = cosine_similarity_loss(y_true, y_pred)\n",
    "    mse = tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "    return cosine + 0.5 * mse  # weight mse if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3571bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - cosine_similarity_loss: 0.5909 - loss: 0.5984 - mae: 0.0828\n",
      "Epoch 2/20\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - cosine_similarity_loss: 0.5529 - loss: 0.5543 - mae: 0.0404\n",
      "Epoch 3/20\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - cosine_similarity_loss: 0.5478 - loss: 0.5490 - mae: 0.0376\n",
      "Epoch 4/20\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - cosine_similarity_loss: 0.5336 - loss: 0.5348 - mae: 0.0376\n",
      "Epoch 5/20\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - cosine_similarity_loss: 0.5193 - loss: 0.5204 - mae: 0.0368\n",
      "Epoch 6/20\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - cosine_similarity_loss: 0.5019 - loss: 0.5030 - mae: 0.0359\n",
      "Epoch 7/20\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - cosine_similarity_loss: 0.4889 - loss: 0.4900 - mae: 0.0353\n",
      "Epoch 8/20\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - cosine_similarity_loss: 0.4829 - loss: 0.4840 - mae: 0.0350\n",
      "Epoch 9/20\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - cosine_similarity_loss: 0.4808 - loss: 0.4819 - mae: 0.0356\n",
      "Epoch 10/20\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - cosine_similarity_loss: 0.4703 - loss: 0.4714 - mae: 0.0357\n",
      "Epoch 11/20\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - cosine_similarity_loss: 0.4649 - loss: 0.4660 - mae: 0.0364\n",
      "Epoch 12/20\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - cosine_similarity_loss: 0.4636 - loss: 0.4648 - mae: 0.0367\n",
      "Epoch 13/20\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - cosine_similarity_loss: 0.4585 - loss: 0.4597 - mae: 0.0373\n",
      "Epoch 14/20\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - cosine_similarity_loss: 0.4565 - loss: 0.4577 - mae: 0.0378\n",
      "Epoch 15/20\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - cosine_similarity_loss: 0.4500 - loss: 0.4513 - mae: 0.0386\n",
      "Epoch 16/20\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - cosine_similarity_loss: 0.4506 - loss: 0.4519 - mae: 0.0392\n",
      "Epoch 17/20\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - cosine_similarity_loss: 0.4481 - loss: 0.4495 - mae: 0.0400\n",
      "Epoch 18/20\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - cosine_similarity_loss: 0.4465 - loss: 0.4480 - mae: 0.0402\n",
      "Epoch 19/20\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - cosine_similarity_loss: 0.4439 - loss: 0.4454 - mae: 0.0412\n",
      "Epoch 20/20\n",
      "\u001b[1m456/456\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - cosine_similarity_loss: 0.4470 - loss: 0.4485 - mae: 0.0413\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1db812a2e10>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def create_mlp(input_dim, output_dim):\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(input_dim,)),\n",
    "        layers.Dense(1024, activation='relu'),\n",
    "        layers.Dense(1024, activation='relu'),\n",
    "        layers.Dense(output_dim)  # Linear activation for regression\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss=hybrid_loss, metrics=['mae',cosine_similarity_loss])\n",
    "    return model\n",
    "\n",
    "model = create_mlp(X_train.shape[1], Y_train.shape[1])\n",
    "model.fit(X_train, Y_train, batch_size=16, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8f01d1ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - cosine_similarity_loss: 0.5800 - loss: 0.5813 - mae: 0.0391\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5787839293479919, 0.03891289606690407, 0.5774489045143127]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "faec8cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_strategies = np.array(list(set(data_pair[:,1])))\n",
    "# get the encodings for the unique strategies via indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "06f0b81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "locs = [np.where(data_pair[:,1] == strategy)[0][0] for strategy in unique_strategies]\n",
    "unique_embds = Y[locs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "761124d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_strategy(query: str|list[str], verbose=False)->str:\n",
    "    x = tokenizer.batch_encode_plus(\n",
    "        query,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        return_tensors='np'\n",
    "    )\n",
    "    x = x['input_ids'] / tokenizer.vocab_size\n",
    "    pred_emb = model.predict(x)\n",
    "    input_norm = pred_emb / np.linalg.norm(pred_emb, axis=1, keepdims=True)\n",
    "    database_norm = Y / np.linalg.norm(Y, axis=1, keepdims=True)\n",
    "    cos_sim = np.dot(input_norm, database_norm.T)\n",
    "    nearest_indices = np.argmax(cos_sim, axis=1).astype(int)\n",
    "    if verbose:\n",
    "        print(f\"Nearest indices: {nearest_indices}\")\n",
    "        print(\"Similarities:\",[cos_sim[i,nearest_indices[i]] for i in range(len(nearest_indices)) ] )\n",
    "    return data_pair[nearest_indices, 1], cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ad4abc5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m285/285\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n"
     ]
    }
   ],
   "source": [
    "out, sim = predict_strategy(data_pair[:,0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc9eedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
